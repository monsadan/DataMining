#testing 5 predict models with 10 kfold validation
data= example1
data$rown= 1:nrow(data)%% 10
AccA = 0
PreA = 0
RecA = 0
AccB = 0
PreB = 0
RecB = 0
AccC = 0
PreC = 0
RecC = 0
AccD = 0
PreD = 0
RecD = 0
AccE = 0
PreE = 0
RecE = 0
for (val in 0:9) {
test = subset(data, rown==val)
train = subset(data, rown!=val)

#a <- rpart(class~var1+var2, train)
b <- naiveBayes(as.factor(class)~var1+var2, train)
#c <- kmeans(data[,1:2], 3)
#d <-svm(class~var1+var2, train)
#e <-neuralnet(data=train, class~var1+var2, hidden=4)

apred <- predict(a, test[,1:2])
atab <- table(round(apred, 0), test[,3])
AccA <- AccA + (atab[1]+atab[4])/1000
RecA <- RecA + (atab[1]/(atab[1]+atab[2]))
PreA <- PreA + (atab[1]/(atab[1]+atab[3]))

bpred <- predict(b, test[,1:2])
bpred < sapply(bpred, round, digits=0)
bpred <- as.factor(bpred)
btab <- table(bpred, test[,3])
AccB <- AccB + (btab[1]+btab[4])/1000
RecB <- RecB + (btab[1]/(btab[1]+btab[2]))
PreB <- PreB + (btab[1]/(btab[1]+btab[3]))

cpred <- predict(c, test[,1:2])
ctab <- table(round(cpred, 0), test[,3])
AccC <- AccC + (ctab[1]+ctab[4])/1000
RecC <- RecC + (ctab[1]/(ctab[1]+ctab[2]))
PreC <- PreC + (ctab[1]/(ctab[1]+ctab[3]))

dpred <- predict(d, test[,1:2])
dtab <- table(round(dpred, 0), test[,3])
AccD <- AccD + (dtab[1]+dtab[4])/1000
RecD <- RecD + (dtab[1]/(dtab[1]+dtab[2]))
PreD <- PreD + (dtab[1]/(dtab[1]+dtab[3]))

epred <- predict(e, test[,1:2])
etab <- table(round(epred, 0), test[,3])
AccE <- AccE + (etab[1]+etab[4])/1000
RecE <- RecE + (etab[1]/(etab[1]+etab[2]))
PreE <- PreE + (etab[1]/(etab[1]+etab[3]))

}
AccA <- AccA / 10
RecA <- RecA / 10
PreA <- PreA /10
atab
AccA
RecA
PreA


AccB <- AccB / 10
RecB <- RecB / 10
PreB <- PreB /10
btab
AccB
RecB
PreB

AccC <- AccC / 10
RecC <- RecC / 10
PreC <- PreC /10
dtab
AccC
RecC
PreC

AccD <- AccD / 10
RecD <- RecD / 10
PreD <- PreD /10
dtab
AccD
RecD
PreD

AccE <- AccE / 10
RecE <- RecE / 10
PreE <- PreE /10
etab
AccE
RecE
PreE



#Course of dimentionality
example4 <- read.delim("C:/Users/dmmpc/Desktop/GVSU/Winter_2020/CIS635_DataMining/HW_Project/example4.txt")
data2 = example4
AccT <- c()
AccN <- c()
AccK <- c()
for (val in 1:20) {
newc = sapply(runif(10000,1,100), round, digits=0)
data2= cbind(newc, data2)
names(data2)[1] <- val

set.seed(1)
dt = sort(sample(nrow(data2), nrow(data2)*.7))
 
train<-data2[dt,]
test<-data2[-dt,]

tree <- rpart(class~., train)
naive <- naiveBayes(as.factor(class)~., train)
Knn <- knn(train[,1:(val+2)], test[,1:(val+2)],train[,val+3] , k = 3)

Tpred <- predict(tree, test[,1:(val+2)])
Tpred <- sapply(Tpred, round, digits=0)
Ttab <- table(Tpred, test[,(val+3)])
CalT <- (Ttab[1]+Ttab[4])/3000

Npred <- predict(naive, test[,1:(val+2)])
Ntab <- table(Npred, test[,(val+3)])
CalN <- (Ntab[1]+Ntab[4])/3000


Ktab <- table(Knn, test[,(val+3)])
CalK  <- (Ktab[1]+Ktab[4])/3000

 
AccT <- c(AccT, CalT)
AccN <- c(AccN, CalN)
AccK <- c(AccK, CalK)

}

plot(AccT,type = "o",col = "red", ylim = c(0.7,1))
lines(AccN, type = "o", col = "blue")
lines(AccK, type = "o", col = "green")
