#testing 5 predict models with 10 kfold validation
example4 <- read.delim("C:/Users/dmmpc/Desktop/GVSU/Winter_2020/CIS635_DataMining/HW_Project/example4.txt")
data= example4
data$rown= 1:nrow(data)%% 10
#accA = 0
#preA = 0
#recA = 0
#accB = 0
#preB = 0
#recB = 0
#accC = 0
#preC = 0
#recC = 0
#accD = 0
#preD = 0
#recD = 0
accE = 0
preE = 0
recE = 0
for (val in 0:9) {
test = subset(data, rown==val)
train = subset(data, rown!=val)
#   a <- rpart(class~var1+var2, train)
#    b <- naiveBayes(as.factor(class)~var1+var2, data=train)
 #   c <- knn(train[,1:2], test[,1:2],train[,3] , k = 3)
  #  d <-svm(class~var1+var2, train)
    e <-neuralnet(data=train, factor(class)~var1+var2, hidden=4)
    
 #   pred <- predict(a,test[,1:2])
  #  table <- table(round(pred,0),test[,3])
   # accA  <- accA + (table[1] + table[4]) / 1000
    #recA  <- recA + (table[1]/ (table[1] + table[2]))
    #preA <- preA + (table[1]/ (table[1] + table[3]))

#    pred <- predict(b,test)
#    table <- table(pred,test$class)
#    accB  <- accB + (table[1] + table[4]) / 1000
#    recB  <- recB + (table[1]/ (table[1] + table[2]))
#    preB <- preB + (table[1]/ (table[1] + table[3]))

#    table <- table(c,test[,3])
#    accC  <- accC + (table[1] + table[4]) / 1000
#    recC  <- recC + (table[1]/ (table[1] + table[2]))
#    preC <- preC + (table[1]/ (table[1] + table[3]))

#    pred <- predict(d,test[,1:2])
#    table <- table(round(pred,0),test[,3])
#    accD  <- accD + (table[1] + table[4]) / 1000
#    recD  <- recD + (table[1]/ (table[1] + table[2]))
#    preD <- preD + (table[1]/ (table[1] + table[3]))

#    pred <- round(predict(e,test[,1:2]),0)
#    table <- table(round(pred,0),test[,3])
#    accE  <- accE + (table[1] + table[4]) / 1000
#    recE  <- recE + (table[1]/ (table[1] + table[2]))
#    preE <- preE + (table[1]/ (table[1] + table[3]))
}
#accA/10
#recA/10
#preA/10

#accB/10
#recB/10
#preB/10

#accC/10
#recC/10
#preC/10

#accD/10
#recD/10
#preD/10

accE/10
recE/10
preE/10



#Course of dimentionality
example4 <- read.delim("C:/Users/dmmpc/Desktop/GVSU/Winter_2020/CIS635_DataMining/HW_Project/example4.txt")
data2 = example4
AccT <- c()
AccN <- c()
AccK <- c()
for (val in 1:20) {
newc = sapply(runif(10000,1,100), round, digits=0)
data2= cbind(newc, data2)
names(data2)[1] <- val

set.seed(1)
dt = sort(sample(nrow(data2), nrow(data2)*.7))
 
train<-data2[dt,]
test<-data2[-dt,]

tree <- rpart(class~., train)
naive <- naiveBayes(as.factor(class)~., train)
Knn <- knn(train[,1:(val+2)], test[,1:(val+2)],train[,val+3] , k = 3)

Tpred <- predict(tree, test[,1:(val+2)])
Tpred <- sapply(Tpred, round, digits=0)
Ttab <- table(Tpred, test[,(val+3)])
CalT <- (Ttab[1]+Ttab[4])/3000

Npred <- predict(naive, test[,1:(val+2)])
Ntab <- table(Npred, test[,(val+3)])
CalN <- (Ntab[1]+Ntab[4])/3000


Ktab <- table(Knn, test[,(val+3)])
CalK  <- (Ktab[1]+Ktab[4])/3000

 
AccT <- c(AccT, CalT)
AccN <- c(AccN, CalN)
AccK <- c(AccK, CalK)

}

plot(AccT,type = "o",col = "red", ylim = c(0.7,1))
lines(AccN, type = "o", col = "blue")
lines(AccK, type = "o", col = "green")
